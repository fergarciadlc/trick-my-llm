# Config-driven experiment spec (Option 2 inline expected answer).

base_url: "https://api.groq.com/openai/v1"
api_key_env: "GROQ_API_KEY"

models:
  - name: "llama-3.1-70b-versatile"
    provider: "groq"
    model_params:
      temperature: 0.0
      max_tokens: 256
  # - name: "llama-3.1-8b-instant"
  #   provider: "groq"
  #   model_params:
  #     temperature: 0.0
  #     max_tokens: 256

defaults:
  temperature: 0.0
  max_tokens: 256
  top_p: 1.0
  timeout_s: 60
  # seed: 123  # uncomment if the target model supports it

replications: 1

scenarios:
  - id: "contradictory_table"
    system_prompt: "prompts/system__use_provided_data_only.md"
    user_prompt_template: "prompts/scenarios/user__contradictory_table.md.j2"
  # - id: "two_sources_hierarchy"
  #   system_prompt: "prompts/system__use_provided_data_only.md"
  #   user_prompt_template: "prompts/scenarios/user__two_sources_hierarchy.md.j2"
  # - id: "recompute_mean"
  #   system_prompt: "prompts/system__use_provided_data_only.md"
  #   user_prompt_template: "prompts/scenarios/user__recompute_mean.md.j2"
  # - id: "custom_units"
  #   system_prompt: "prompts/system__use_provided_data_only.md"
  #   user_prompt_template: "prompts/scenarios/user__custom_units.md.j2"
